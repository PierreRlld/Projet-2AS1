{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import bs4\n",
    "import pandas\n",
    "from urllib import request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlfoot=\"https://fr.wikipedia.org/wiki/Liste_des_mouvements_litt%C3%A9raires\"\n",
    "request_text= request.urlopen(urlfoot).read()\n",
    "page = bs4.BeautifulSoup(request_text, \"lxml\") #qui est bien une page html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# === On get la page d'intérêt ===\n",
    "Le but de ce notebook est d'obtenir les données contenues dans les pages wikipédia des différents mouvements littéraires afin d'effectuer par la suite des prédictions associant des auteurs à leur mouvement.\n",
    "On veut donc, dans la page wikipedia \"Liste des mouvements littéraires\" récupérer les liens correspondant à des mouvements littéraires, visiter ces pages et récuperer leur contenu.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li><a href=\"/wiki/Humanisme_de_la_Renaissance\" title=\"Humanisme de la Renaissance\">Humanisme de la Renaissance</a></li>, <li>La <a href=\"/wiki/Pl%C3%A9iade_(XVIe_si%C3%A8cle)\" title=\"Pléiade (XVIe siècle)\">Pléiade</a></li>, <li><a class=\"mw-redirect\" href=\"/wiki/Picaresque\" title=\"Picaresque\">Picaresque</a></li>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test=page.findAll(\"ul\")\n",
    "L=[]\n",
    "for res in test :\n",
    "  test2 = res.findAll(\"li\")\n",
    "  L.append(test2)\n",
    "\n",
    "print(L[15])\n",
    "\n",
    "#On vérifier la forme des balises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'> Humanisme de la Renaissance\n",
      "<class 'bs4.element.Tag'> Pléiade (XVIe siècle)\n",
      "<class 'bs4.element.Tag'> Picaresque\n"
     ]
    }
   ],
   "source": [
    "for res in L[15]:\n",
    "    try :\n",
    "       res2=res.find(\"a\")\n",
    "       print(type(res), res2.get(\"title\"))    #https://stackoverflow.com/questions/57395509/get-item-from-bs4-element-tag\n",
    "    except :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "base=page.findAll(\"ul\")         \n",
    "L=[]\n",
    "for ul in base :\n",
    "  L.append(ul.findAll(\"li\"))\n",
    "#print(L)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultat=[]\n",
    "for i in range(len(L)) :      #On peut pas faire direct for li in L à cause du type\n",
    "    for li in L[i]:\n",
    "        try :\n",
    "            inter=li.find(\"a\") \n",
    "            if type(inter.get(\"title\"))==str:\n",
    "                s=inter.get('href')\n",
    "                if s[:5]=='/wiki':\n",
    "                    resultat.append(s)\n",
    "      #print(type(inter))\n",
    "        except:                   #On peut avoir type None pour inter \n",
    "            pass\n",
    "len(resultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultat correspond à la liste des liens récupérés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mouvements = resultat[13:70]\n",
    "# tous les liens de résultat ne correspondent pas à des mouvements littéraires. On regarde donc à la main où commence et où finissent les liens intéressants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Troubadour'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultat[15][6:] #exemple de ce que l'on récupère"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'https://fr.wikipedia.org' #base de l'url wiki "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée deux dictionnaires où l'on stocke les données intéressantes\n",
    "# dico_bs stocke les pages html de chaque mouvement\n",
    "# dico_txt stocke le texte \n",
    "\n",
    "dico_bs = {}\n",
    "dico_txt = {}\n",
    "for link in res_mouvements:\n",
    "    url_ = wiki_url + link\n",
    "    request_text= request.urlopen(url_).read()\n",
    "    dico_bs[link[6:]]=bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "    txt=''\n",
    "    page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "    base=page.findAll(\"p\")         \n",
    "    for h2 in base :\n",
    "        for string in h2.strings:\n",
    "            txt+=string\n",
    "    dico_txt[link[6:]]=txt\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dico_txt['Troubadour'] exemple de résultat provisoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup = bs4.BeautifulSoup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'findAll'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-554bd43da7ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtxt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m         \u001b[1;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2160\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   2161\u001b[0m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'findAll'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "txt=''\n",
    "base=test.findAll(\"p\")         \n",
    "for h2 in base :\n",
    "    for string in h2.strings:\n",
    "        txt+=string\n",
    "   \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt\n",
    "#L[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-36-f7a31e2fb7f3>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-f7a31e2fb7f3>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    # print(string)\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "soup = BeautifulSoup(doc, \"html.parser\") \n",
    "  \n",
    "tag = soup.body \n",
    "  \n",
    "for string in tag.strings: \n",
    "   # print(string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4682"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dico_txt['Absurde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "# dico de vocab pr chaque texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cpl in dico_txt.items():\n",
    " #   key,value=cpl\n",
    "  #  print(key),print(len(value))\n",
    "  #Visualisation des résultats pour vérifier que les données récupérées ne sont pas aberrantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dico_txt['Picaresque']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va désormais traiter les données afin de les rendre exploitables. On va donc produire, à partir des textes bruts scrapés, produire des textes desquels seront retirés les \"stopwords\", c'est-à-dire les mots n'apportant pas de sens et de complexité au texte. On considère qu'ils sont inutiles car ne permettront pas de différencier les mouvements entre eux. On va également lemmatiser le texte, c'est-à-dire considérer la racine/le radical de chaque mot plutôt que le mot lui même, encore une fois pour nous intéresser au sens avant tout (on considère, par exemple,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e8f10ac583de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fr_core_news_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop=stopwords.words('french')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(s):\n",
    "    doc = nlp(s)\n",
    "    L=[]\n",
    "    for token in doc: \n",
    "        s_t = str(token) #on récupère le token\n",
    "        alphanum=True # on initialise cette variable booléenne\n",
    "        if not s_t in stop:\n",
    "            for char in s_t:\n",
    "                alphanum = char.isalnum() and alphanum \n",
    "            if alphanum:            #si chaque char du token est alphanumérique, on le recupère\n",
    "                L.append(token.lemma_) # et on ajoute sa forme lemmatisée\n",
    "    L=np.array(L)\n",
    "    return ' '.join(L) # on récupère le texte en espaçant les lemmes de ' '\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Litt%C3%A9rature_hagiographique\n",
      "11557\n",
      "Chanson_de_geste\n",
      "12214\n",
      "Troubadour\n",
      "15412\n",
      "Roman_courtois\n",
      "3204\n",
      "Fabliau\n",
      "8626\n",
      "Grands_rh%C3%A9toriqueurs\n",
      "5366\n",
      "Po%C3%A9sie_courtoise\n",
      "1716\n",
      "Humanisme_de_la_Renaissance\n",
      "29058\n",
      "Pl%C3%A9iade_(XVIe_si%C3%A8cle)\n",
      "8652\n",
      "Picaresque\n",
      "5092\n",
      "Litt%C3%A9rature_baroque\n",
      "8374\n",
      "Burlesque\n",
      "7865\n",
      "Classicisme\n",
      "24412\n",
      "Qui%C3%A9tisme\n",
      "8922\n",
      "Pr%C3%A9ciosit%C3%A9\n",
      "3389\n",
      "Moralisme\n",
      "20133\n",
      "Illuminisme\n",
      "15969\n",
      "Roman_gothique\n",
      "8076\n",
      "Pr%C3%A9romantisme\n",
      "3174\n",
      "Libertin\n",
      "9416\n",
      "Lumi%C3%A8res_(philosophie)\n",
      "39936\n",
      "Romantisme\n",
      "18512\n",
      "R%C3%A9alisme_(litt%C3%A9rature)\n",
      "17681\n",
      "Naturalisme_(litt%C3%A9rature)\n",
      "6691\n",
      "Parnasse_(litt%C3%A9rature)\n",
      "9562\n",
      "Symbolisme_(art)\n",
      "51191\n",
      "%C3%89cole_romane\n",
      "2044\n",
      "D%C3%A9cadentisme\n",
      "9184\n",
      "Esth%C3%A9tisme\n",
      "785\n",
      "Absurde\n",
      "4682\n",
      "Surr%C3%A9alisme\n",
      "25045\n",
      "Oulipo\n",
      "9823\n",
      "Futurisme\n",
      "11262\n",
      "Courant_de_conscience\n",
      "4054\n",
      "Dada%C3%AFsme\n",
      "20677\n",
      "Populisme_(litt%C3%A9rature)\n",
      "1228\n",
      "R%C3%A9alisme_socialiste_en_France\n",
      "14123\n",
      "Renaissance_litt%C3%A9raire_catholique_en_France\n",
      "5652\n",
      "Litt%C3%A9rature_prol%C3%A9tarienne\n",
      "17017\n",
      "Roman_jdanovien_fran%C3%A7ais\n",
      "4994\n",
      "Existentialisme\n",
      "33426\n",
      "N%C3%A9or%C3%A9alisme_(art)\n",
      "2194\n",
      "Objectivisme_(litt%C3%A9rature)\n",
      "4293\n",
      "%C3%89cole_de_Rochefort\n",
      "3638\n",
      "Th%C3%A9%C3%A2tre_de_l%27absurde\n",
      "6492\n",
      "Lettrisme\n",
      "14765\n",
      "Lost_Generation\n",
      "1777\n",
      "Beat_Generation\n",
      "12422\n",
      "Nouveau_roman\n",
      "29955\n",
      "Autofiction\n",
      "6884\n",
      "R%C3%A9alisme_magique\n",
      "17487\n",
      "Nouvelle_fiction\n",
      "4417\n",
      "Imagisme\n",
      "9979\n",
      "R%C3%A9alisme_hyst%C3%A9rique\n",
      "2679\n",
      "Post-humanisme\n",
      "11567\n",
      "N%C3%A9gritude\n",
      "6142\n",
      "Sacra%C3%AFsme\n",
      "2341\n"
     ]
    }
   ],
   "source": [
    "dico_txt_clean={}\n",
    "for cpl in dico_txt.items():\n",
    "    key,value=cpl\n",
    "    #print(key),print(len(value))\n",
    "    dico_txt_clean[key]=clean_txt(value)\n",
    "\n",
    "    #On récupère le texte \"cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dico_txt_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6a95fc76b94f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdico_txt_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Absurde'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dico_txt_clean' is not defined"
     ]
    }
   ],
   "source": [
    "test =dico_txt_clean['Absurde']\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word,doc):\n",
    "    L=doc.split()\n",
    "    c=0\n",
    "    n=len(L)\n",
    "    for k in range(n):\n",
    "        if L[k]==word:\n",
    "            c+=1\n",
    "    return c/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word,corpus):\n",
    "    n=len(corpus)\n",
    "    c=0\n",
    "    for couple in corpus.items():\n",
    "        key,value=couple\n",
    "        if word in value:\n",
    "            c+=1\n",
    "    return log(n/c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(word,doc,corpus):\n",
    "    return tf(word,doc)*idf(word,corpus)\n",
    "\n",
    "    #la mesure tf-idf permet de faire ressortir les mots signficatifs. Ici, on regarde, pour chaque mot, sa valeur relative dans le document (qui sera le contenu texte de la page wikipédia du mouvement littéraire) par rapport au corpus (l'ensemble des pages des mouvements littéraires)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09315690201130325"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf('absurde',test,dico_txt_clean) #exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_important_words(n,doc,corpus):\n",
    "    L=doc.split()\n",
    "    m=len(L)\n",
    "    M=[]\n",
    "    vocab=[]\n",
    "    for k in range(m):\n",
    "        if not L[k] in vocab:\n",
    "            M.append((tf_idf(L[k],doc,corpus),k))\n",
    "            vocab.append(L[k])\n",
    "    M.sort()\n",
    "    M.reverse()\n",
    "    return [L[M[k][1]] for k in range(n)] \n",
    "    #Le but est de faire ressortir les n mots les plus importants pour la mesure tf-idf pour chaque mouvement, relativement aux autres.\n",
    "    # On considère que ces mots permettent de faire ressortir l'essence du mouvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_important_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3fde18cb0149>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mn_important_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdico_txt_clean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#un exemple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Pour l'absurde, les mots représentatifs sont les suivants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_important_words' is not defined"
     ]
    }
   ],
   "source": [
    "n_important_words(10,test,dico_txt_clean) #un exemple\n",
    "#Pour l'absurde, les mots représentatifs sont les suivants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Mouvements = pd.DataFrame(columns=['mouvement','texte','texte_lem','tf_idf'])\n",
    "L=list(dico_txt.keys())\n",
    "\n",
    "Mouvements['mouvement']=L\n",
    "Mouvements['texte'] = [dico_txt[mov] for mov in L]\n",
    "Mouvements['texte_lem']=[dico_txt_clean[mov] for mov in L]\n",
    "Mouvements['tf_idf']=[n_important_words(50,dico_txt_clean[mov],dico_txt_clean) for mov in L]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient finalement le dataframe Mouvements_wiki_cleaned.csv qui contient les mouvements littéraires scrapés, les textes scrapés du contenu de chaque page, le \"cleaning\" de chaque texte (cleaning et lemmatisation), et les 50 mots avec les plus gros scores tf-idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les pages wikipedia contenant cependant des textes inutiles \"Cette page wikipédia est incomplète\" ou \"Pour les articles homonymes, voir...\". Il faut donc retirer ces parties sans intérêt. Cela a été fait à la main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for k in range(57):\n",
    " #   print(Mouvements['texte'][k][:100],k)\n",
    "\n",
    " #Pour chaque mouvement, on affiche donc le texte et on essaie de le découper au bon endroit.\n",
    "\n",
    " # Mouvements['texte']=Mouvements['texte'].apply(lambda texte: texte[54:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouvements['texte']=Mouvements['texte'].apply(lambda s: s.replace('\\n',''))\n",
    "\n",
    "# Mouvements['texte']=Mouvements['texte'].apply(lambda s: s.replace('\\'',''))\n",
    "\n",
    "#autres traitements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouvements.to_csv('Mouvements_wiki_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
